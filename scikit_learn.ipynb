{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOOChH4d0xtuYXQgEyE85d3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WinstonMuijs/Natural-Language-Processing/blob/main/scikit_learn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M0dzPMgcmDrR"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 'Bouwen van woord-vectoren in scikit learn'"
      ],
      "metadata": {
        "id": "UA5rftvRkzfA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   verzamelen en preprocessen data\n",
        "2.   bepalen label\n",
        "3.   splitten van data in train en test\n",
        "4.   Features van de tekst eruit halen in de vorm van een BOW-vector, als input, om voorspellingen te kunnen doen.\n",
        "5.   evaluatie getraind model.\n",
        "\n"
      ],
      "metadata": {
        "id": "5jFyu-QPlZX9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importeren van Pandas en Scikit Learn modules"
      ],
      "metadata": {
        "id": "I9n7-rAugjTb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "JxAAM82DclLX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "FYRaQKlqcqvo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "vnh0IYoBc1vn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creeren van een DataFrame op basis van een csv-file"
      ],
      "metadata": {
        "id": "DaTpExMTgonH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"fake_or_real_news.csv\")\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnvhphQ3c-66",
        "outputId": "a3e572e3-1689-489d-9d93-fb35d2bba41b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Unnamed: 0                                              title  \\\n",
            "0           8476                       You Can Smell Hillary’s Fear   \n",
            "1          10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
            "2           3608        Kerry to go to Paris in gesture of sympathy   \n",
            "3          10142  Bernie supporters on Twitter erupt in anger ag...   \n",
            "4            875   The Battle of New York: Why This Primary Matters   \n",
            "...          ...                                                ...   \n",
            "6330        4490  State Department says it can't find emails fro...   \n",
            "6331        8062  The ‘P’ in PBS Should Stand for ‘Plutocratic’ ...   \n",
            "6332        8622  Anti-Trump Protesters Are Tools of the Oligarc...   \n",
            "6333        4021  In Ethiopia, Obama seeks progress on peace, se...   \n",
            "6334        4330  Jeb Bush Is Suddenly Attacking Trump. Here's W...   \n",
            "\n",
            "                                                   text label  \n",
            "0     Daniel Greenfield, a Shillman Journalism Fello...  FAKE  \n",
            "1     Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  \n",
            "2     U.S. Secretary of State John F. Kerry said Mon...  REAL  \n",
            "3     — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  \n",
            "4     It's primary day in New York and front-runners...  REAL  \n",
            "...                                                 ...   ...  \n",
            "6330  The State Department told the Republican Natio...  REAL  \n",
            "6331  The ‘P’ in PBS Should Stand for ‘Plutocratic’ ...  FAKE  \n",
            "6332   Anti-Trump Protesters Are Tools of the Oligar...  FAKE  \n",
            "6333  ADDIS ABABA, Ethiopia —President Obama convene...  REAL  \n",
            "6334  Jeb Bush Is Suddenly Attacking Trump. Here's W...  REAL  \n",
            "\n",
            "[6335 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gJghgY_hgu7h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "De labels vormen de ouput van het model. Het zijn de labels die je wilt dat het model levert of voorspelt. "
      ],
      "metadata": {
        "id": "meIPvdkOhhqx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = df.label"
      ],
      "metadata": {
        "id": "IHKgpZcidfFu"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Je splitst je dataset in een train- en testset en je zegt dat de input-waarden van het model de tekstfeatures (rijen) zijn van de DataFrame, de output waarden y, en dat je de set verdeelt voor 33% in test en 66% in train data."
      ],
      "metadata": {
        "id": "Ja6lGV8fh5ea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test , y_train, y_test = train_test_split(df['text'], y, test_size = 0.33, random_state = 53)"
      ],
      "metadata": {
        "id": "o1JkpFnmdv8v"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tekst wordt omgezet in een Bag of Words vector, net als een corpus in gensim. Een corpus transformeert een document in een Bag of Words door token id's en de frequentie van de woorden - tokens - in het document te nemen en deze in een lijst met tuples weer te geven. [[(id, freq,), (id, freq) ,(id, freq) ]] \n",
        "CountVectorizer doet ongeveer hetzelfde en een bag of words vector, waarbij elke token een feature is."
      ],
      "metadata": {
        "id": "xDApu7hJiqd8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count_vectorizer = CountVectorizer(stop_words = 'english')"
      ],
      "metadata": {
        "id": "0zO9_Et3eOJZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Er wordt nu een BOW-vector gemaakt van de trainingsdata: mapping van de woorden met id's en vectoren hoeveel x een woord verschijnt in de tekst."
      ],
      "metadata": {
        "id": "jI97JNXRj2ZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count_train = count_vectorizer.fit_transform(X_train.values)"
      ],
      "metadata": {
        "id": "xM0CiprpeXlF"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ook voor de test data"
      ],
      "metadata": {
        "id": "UU7LaMf8kXag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count_test = count_vectorizer.transform(X_test.values)"
      ],
      "metadata": {
        "id": "OgpGQ5-NfzEt"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TmgoaI6Rka_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the first 5 features of the count_vectorizer\n",
        "print(count_vectorizer.get_feature_names_out()[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nrjBq87f9El",
        "outputId": "9d8c4c62-174b-4642-9dd2-50a839b1b428"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['00' '000' '0000' '00000031' '000035']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TfidfVectorizer gebruikt ipv CountVectorizer"
      ],
      "metadata": {
        "id": "L6lX9eyEUZeD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize a TfidfVectorizer object: tfidf_vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
        "\n",
        "# Transform the training data: tfidf_train \n",
        "tfidf_train = tfidf_vectorizer.fit_transform(X_train.values)\n",
        "\n",
        "# Transform the test data: tfidf_test \n",
        "tfidf_test = tfidf_vectorizer.transform(X_test.values)\n",
        "\n",
        "# Print the first 10 features\n",
        "print(tfidf_vectorizer.get_feature_names_out()[:10])\n",
        "\n",
        "# Print the first 5 vectors of the tfidf training data\n",
        "print(tfidf_train.A[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2TwnG0RUZDE",
        "outputId": "4b588e18-1202-4d7d-85f3-c1cac0ab9986"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['00' '000' '0000' '00000031' '000035' '00006' '0001' '0001pt' '000ft'\n",
            " '000km']\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Het creeren van DataFrames om te zien hoe de (woord)-vectoren eruit zien."
      ],
      "metadata": {
        "id": "g1MZw03BTGrx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the CountVectorizer DataFrame: count_df\n",
        "count_df = pd.DataFrame(count_train.A, columns=count_vectorizer.get_feature_names_out())\n",
        "\n",
        "# Create the TfidfVectorizer DataFrame: tfidf_df\n",
        "tfidf_df = pd.DataFrame(tfidf_train.A, columns=tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "# Print the head of count_df\n",
        "print(count_df.head())\n",
        "\n",
        "# Print the head of tfidf_df\n",
        "print(tfidf_df.head())\n",
        "\n",
        "# Calculate the difference in columns: difference\n",
        "difference = set(count_df.columns) - set(tfidf_df.columns)\n",
        "print(difference)\n",
        "\n",
        "# Check whether the DataFrames are equal\n",
        "print(count_df.equals(tfidf_df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuKGKCenSywN",
        "outputId": "11997fa8-666c-4d28-ae2b-78dbfc53078e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   00  000  0000  00000031  000035  00006  0001  0001pt  000ft  000km  ...  \\\n",
            "0   0    0     0         0       0      0     0       0      0      0  ...   \n",
            "1   0    0     0         0       0      0     0       0      0      0  ...   \n",
            "2   0    0     0         0       0      0     0       0      0      0  ...   \n",
            "3   0    0     0         0       0      0     0       0      0      0  ...   \n",
            "4   0    0     0         0       0      0     0       0      0      0  ...   \n",
            "\n",
            "   حلب  عربي  عن  لم  ما  محاولات  من  هذا  والمرضى  ยงade  \n",
            "0    0     0   0   0   0        0   0    0        0      0  \n",
            "1    0     0   0   0   0        0   0    0        0      0  \n",
            "2    0     0   0   0   0        0   0    0        0      0  \n",
            "3    0     0   0   0   0        0   0    0        0      0  \n",
            "4    0     0   0   0   0        0   0    0        0      0  \n",
            "\n",
            "[5 rows x 56922 columns]\n",
            "    00  000  0000  00000031  000035  00006  0001  0001pt  000ft  000km  ...  \\\n",
            "0  0.0  0.0   0.0       0.0     0.0    0.0   0.0     0.0    0.0    0.0  ...   \n",
            "1  0.0  0.0   0.0       0.0     0.0    0.0   0.0     0.0    0.0    0.0  ...   \n",
            "2  0.0  0.0   0.0       0.0     0.0    0.0   0.0     0.0    0.0    0.0  ...   \n",
            "3  0.0  0.0   0.0       0.0     0.0    0.0   0.0     0.0    0.0    0.0  ...   \n",
            "4  0.0  0.0   0.0       0.0     0.0    0.0   0.0     0.0    0.0    0.0  ...   \n",
            "\n",
            "   حلب  عربي   عن   لم   ما  محاولات   من  هذا  والمرضى  ยงade  \n",
            "0  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  \n",
            "1  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  \n",
            "2  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  \n",
            "3  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  \n",
            "4  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  \n",
            "\n",
            "[5 rows x 56922 columns]\n",
            "set()\n",
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Naive Bayes Classifer from Scikit Learn"
      ],
      "metadata": {
        "id": "JK-NfdhTX1fe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import metrics\n",
        "\n",
        "nb_classifer = MultinomialNB() #creeren een model of classifer"
      ],
      "metadata": {
        "id": "easm51rdX04s"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb_classifer.fit(count_train, y_train) # trainen de classifer/model met de train data.\n",
        "pred = nb_classifer.predict(count_test) # Voorspellen op basis van het getainde classifer/model wat het resultaat is van de test data."
      ],
      "metadata": {
        "id": "2V8vJDE-X6nO"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics.accuracy_score(y_test, pred) # We kijken wat de accuraatheid is van de voorspellen met de test labels/data."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fw9_P8hhZz7Y",
        "outputId": "1a93c334-27f3-49f9-c86d-e607947f11a4"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.893352462936394"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Confusion Matrix"
      ],
      "metadata": {
        "id": "V54uy2_HaQ2j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics.confusion_matrix(y_test, pred, labels=[\"FAKE\",\"REAL\"]) # Om te bepalen hoeveel goede en foute classificaties zijn gedaan.\n",
        "                                                               # 865 goed, 143 fout \"Fake news\" labelling\n",
        "                                                               # 80 fout, 1003 goed \"Real news\" labelling"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1_uOi5iaUrC",
        "outputId": "3c71b54d-a80c-46db-fc51-19508716a4b0"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 865,  143],\n",
              "       [  80, 1003]])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Same with TfidfVectorizer"
      ],
      "metadata": {
        "id": "c_MkvdAYfu64"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize a TfidfVectorizer object: tfidf_vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
        "\n",
        "# Transform the training data: tfidf_train \n",
        "tfidf_train = tfidf_vectorizer.fit_transform(X_train.values)\n",
        "\n",
        "# Transform the test data: tfidf_test \n",
        "tfidf_test = tfidf_vectorizer.transform(X_test.values)\n",
        "\n",
        "# Print the first 10 features\n",
        "print(tfidf_vectorizer.get_feature_names_out()[:10])\n",
        "\n",
        "# Print the first 5 vectors of the tfidf training data\n",
        "print(tfidf_train.A[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNJtxy3NfzCb",
        "outputId": "01ea54d5-4d94-4282-ed32-6bd2efae8672"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['00' '000' '0000' '00000031' '000035' '00006' '0001' '0001pt' '000ft'\n",
            " '000km']\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nb_classifer.fit(tfidf_train, y_train) # trainen de classifer/model met de train data.\n",
        "pred = nb_classifer.predict(tfidf_test)"
      ],
      "metadata": {
        "id": "YI-qmHcqgWQw"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics.accuracy_score(y_test, pred) # Zoals je ziet een minder goede voorspellingsgraad ivm NB"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2cztOVUgkQE",
        "outputId": "83038ff5-cfae-40ce-9eee-389f0cd338cd"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8565279770444764"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metrics.confusion_matrix(y_test, pred, labels=[\"FAKE\",\"REAL\"]) # Meer fouten bij het bepalen van 'Fake news\". "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4I2APtobgt39",
        "outputId": "f5aa85bb-1b19-45bb-8e10-d2fd25b9e0e1"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 739,  269],\n",
              "       [  31, 1052]])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    }
  ]
}